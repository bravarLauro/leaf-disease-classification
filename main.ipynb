{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras import metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from pandas import DataFrame\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_local(new_w,new_h):\n",
    "    train_labels = pd.read_csv(r'C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train.csv',sep=',')\n",
    "    path_loop = r\"C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train_images\\*.*\"\n",
    "    path = r\"C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train_images\"\n",
    "    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n",
    "    numOfFiles = len(onlyfiles)\n",
    "    data = []\n",
    "    for file in tqdm(glob.glob(path_loop)):\n",
    "        a=cv2.imread(file)\n",
    "        name_file = os.path.basename(file)\n",
    "        label = train_labels.loc[train_labels['image_id'] == name_file,'label'].values\n",
    "        #conversion numpy array into rgb image to show\n",
    "        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "        h, w, channels = c.shape\n",
    "        #input size of Resnet architecture\n",
    "        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n",
    "        data.append([frame_rgb,label])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(new_w,new_h):\n",
    "    train_labels = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv', sep=',')\n",
    "    path = '../input/cassava-leaf-disease-classification/train_images'\n",
    "    path_loop = r'../input/cassava-leaf-disease-classification/train_images/*.*'\n",
    "    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n",
    "    numOfFiles = len(onlyfiles)\n",
    "    data = []\n",
    "    for file in tqdm(glob.glob(path_loop)):\n",
    "        a=cv2.imread(file)\n",
    "        name_file = os.path.basename(file)\n",
    "        label = train_labels.loc[train_labels['image_id'] == name_file,'label'].values\n",
    "        #conversion numpy array into rgb image to show\n",
    "        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "        h, w, channels = c.shape\n",
    "        #input size of Resnet architecture\n",
    "        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n",
    "        data.append([frame_rgb,label])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change before submitting to Kaggle from load_data_local to load_data\n",
    "data = load_data_local(224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_modified(data):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for feature, label in data:\n",
    "        x_data.append(feature)\n",
    "        y_data.append(label)\n",
    "        \n",
    "    X_data, X_test, Y_data, Y_test = sklearn.model_selection.train_test_split(x_data, y_data, stratify = y_data, test_size=0.1, random_state=42)\n",
    "    X_model_1, X_aux, Y_model_1, Y_aux = sklearn.model_selection.train_test_split(X_data, Y_data, stratify = Y_data, test_size=0.66, random_state=42)\n",
    "    X_model_2, X_model_3, Y_model_2, Y_model_3 = sklearn.model_selection.train_test_split(X_aux, Y_aux, stratify = Y_aux, test_size=0.5, random_state=42)\n",
    "    return X_test, Y_test, X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(X_data, Y_data, new_h, new_w, channels):\n",
    "    x_train,x_val,y_train,y_val = train_test_split(X_data,Y_data,stratify=Y_data, train_size=0.8, random_state=42)\n",
    "    x_train = np.array(x_train) / 255\n",
    "    x_val = np.array(x_val) / 255\n",
    "    y_train = np.array(y_train)\n",
    "    y_val = np.array(y_val)\n",
    "    x_train = x_train.reshape(-1, new_h, new_w, channels)\n",
    "    x_val = x_val.reshape(-1, new_h, new_w, channels)\n",
    "\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode target variables\n",
    "def encode_target_variable(y_train,y_val):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    y_val = y_val.reshape(-1,1)\n",
    "    y_train_onehot = onehot_encoder.fit_transform(y_train)\n",
    "    y_val_onehot = onehot_encoder.fit_transform(y_val)\n",
    "    return y_train_onehot, y_val_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_1(x_train,x_val):\n",
    "    resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    resnet50.trainable = False # remove if you want to retrain resnet weights\n",
    "    resnet50.summary()\n",
    "    transfer_model_1 = Sequential()\n",
    "    transfer_model_1.add(resnet50)\n",
    "    transfer_model_1.add(Flatten())\n",
    "    transfer_model_1.add(Dense(128, activation='relu'))\n",
    "    transfer_model_1.add(Dropout(0.2))\n",
    "    transfer_model_1.add(Dense(5, activation='softmax'))\n",
    "    x_train_new = x_train\n",
    "    x_val_new = x_val\n",
    "    x_train_new = keras.applications.resnet50.preprocess_input(x_train_new)\n",
    "    x_val_new = keras.applications.resnet50.preprocess_input(x_val_new)\n",
    "    return transfer_model_1, x_train_new, x_val_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2():\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    vgg16.trainable = False # remove if you want to retrain vgg16 weights\n",
    "    vgg16.summary()\n",
    "    transfer_model_2 = Sequential()\n",
    "    transfer_model_2.add(vgg16)\n",
    "    transfer_model_2.add(Flatten())\n",
    "    transfer_model_2.add(Dense(128, activation='relu'))\n",
    "    transfer_model_2.add(Dropout(0.2))\n",
    "    transfer_model_2.add(Dense(5, activation='softmax'))\n",
    "    x_train_new = x_train\n",
    "    x_val_new = x_val\n",
    "    x_train_new = VGG16.preprocess_input(x_train_new)\n",
    "    x_val_new = VGG16.preprocess_input(x_val_new)\n",
    "    return transfer_model_2, x_train_new, x_val_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_3():\n",
    "    inceptionV3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    inceptionV3.trainable = False # remove if you want to retrain rinceptionV3 weights\n",
    "    inceptionV3.summary()\n",
    "    transfer_model_3 = Sequential()\n",
    "    transfer_model_3.add(inceptionV3)\n",
    "    transfer_model_3.add(Flatten())\n",
    "    transfer_model_3.add(Dense(128, activation='relu'))\n",
    "    transfer_model_3.add(Dropout(0.2))\n",
    "    transfer_model_3.add(Dense(5, activation='softmax'))\n",
    "    x_train_new = x_train\n",
    "    x_val_new = x_val\n",
    "    x_train_new = InceptionV3.preprocess_input(x_train_new)\n",
    "    x_val_new = InceptionV3.preprocess_input(x_val_new)\n",
    "    return transfer_model_3, x_train_new, x_val_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot):\n",
    "    \n",
    "    batch_size = batch_size\n",
    "    epochs = epochs\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['categorical_accuracy',keras.metrics.AUC(),keras.metrics.Recall()])\n",
    "\n",
    "    history = model.fit(x_train_new, y_train_onehot, \\\n",
    "                              batch_size=batch_size, epochs=epochs, \\\n",
    "                              validation_split=0.2, verbose=1, shuffle=True, validation_data=(x_val_new, y_val_onehot))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Main function ######################\n",
    "\n",
    "\n",
    "new_h = 224\n",
    "new_w = 224\n",
    "batch_size = 500\n",
    "epochs = 10\n",
    "channels = 3\n",
    "#Divide data into test and training/validation for three different models\n",
    "X_test, Y_test, X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3 = train_test_split_modified(data)\n",
    "training_data = [X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3]\n",
    "#Check that the training datasets are correctly stratified \n",
    "Y_model_1_df = DataFrame(Y_model_1,columns=['labels'])\n",
    "#print(Y_model_1_df.labels.value_counts())\n",
    "Y_model_2_df = DataFrame(Y_model_2,columns=['labels'])\n",
    "#print(Y_model_2_df.labels.value_counts())\n",
    "Y_model_3_df = DataFrame(Y_model_3,columns=['labels'])\n",
    "#print(Y_model_3_df.labels.value_counts())\n",
    "#deaseases explanation: {\"0\": \"Cassava Bacterial Blight (CBB)\", \n",
    "#\"1\": \"Cassava Brown Streak Disease (CBSD)\", \"2\": \"Cassava Green Mottle (CGM)\", \n",
    "#\"3\": \"Cassava Mosaic Disease (CMD)\", \"4\": \"Healthy\"}\n",
    "\n",
    "#visualize each of the classes\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(8):\n",
    "    img = X_model_1[i]\n",
    "    fig.add_subplot(2, 4, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(Y_model_1[i])\n",
    "#Resnet50\n",
    "x_train, x_val, y_train, y_val = data_preparation(training_data[0], training_data[1], new_h, new_w, channels)\n",
    "y_train_onehot, y_val_onehot = encode_target_variable(y_train,y_val)\n",
    "del y_train, y_val\n",
    "resnet50_model, x_train_new, x_val_new = create_model_1(x_train,x_val)\n",
    "del x_train,x_val\n",
    "gc.collect()\n",
    "resnet50_model = train_model(resnet50_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot)\n",
    "#vgg16\n",
    "x_train, x_val, y_train, y_val = data_preparation(training_data[2], training_data[3], new_h, new_w, channels)\n",
    "y_train_onehot, y_val_onehot = encode_target_variable(y_train,y_val)\n",
    "vgg16_model, x_train_new, x_val_new = create_model_2(x_train,x_val)\n",
    "gc.collect()\n",
    "vgg16_model = train_model(vgg16_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot)\n",
    "#InceptionV3\n",
    "x_train, x_val, y_train, y_val = data_preparation(training_data[4], training_data[5], new_h, channels)\n",
    "y_train_onehot, y_val_onehot = encode_target_variable(y_train,y_val)\n",
    "inception_model, x_train_new, x_val_new = create_model_3(x_train,x_val)\n",
    "gc.collect()\n",
    "inception_model = train_model(inception_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
