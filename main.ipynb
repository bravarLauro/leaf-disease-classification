{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pylab as plt\nimport glob\nimport os\nimport random\nimport keras\nfrom keras import backend as K\nfrom keras import layers\nfrom keras import metrics\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.models import Sequential, Model\nfrom keras.utils import plot_model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport cv2\nfrom tqdm import tqdm\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom collections import Counter\nfrom pandas import DataFrame\nimport gc\nfrom sklearn.utils import class_weight\nfrom sklearn.utils.class_weight import compute_class_weight","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data_local(new_w,new_h):\n    train_labels = pd.read_csv(r'C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train.csv',sep=',')\n    path_loop = r\"C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train_images\\*.*\"\n    path = r\"C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train_images\"\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        label = train_labels.loc[train_labels['image_id'] == name_file,'label'].values\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        data.append([frame_rgb,label])\n    return data","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(new_w,new_h):\n    train_labels = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv', sep=',')\n    path = '../input/cassava-leaf-disease-classification/train_images'\n    path_loop = r'../input/cassava-leaf-disease-classification/train_images/*.*'\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        label = train_labels.loc[train_labels['image_id'] == name_file,'label'].values\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        data.append([frame_rgb,label])\n    return data\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change before submitting to Kaggle from load_data_local to load_data\nIMG_SIZE_X = 128\nIMG_SIZE_Y = 128\ndata = load_data(IMG_SIZE_X, IMG_SIZE_Y)","execution_count":5,"outputs":[{"output_type":"stream","text":"100%|██████████| 21397/21397 [06:53<00:00, 51.75it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split_modified(data):\n    x_data = []\n    y_data = []\n    \n    for feature, label in data:\n        x_data.append(feature)\n        y_data.append(label)\n        \n    X_data, X_test, Y_data, Y_test = sklearn.model_selection.train_test_split(x_data, y_data, stratify = y_data, test_size=0.1, random_state=42)\n    X_model_1, X_aux, Y_model_1, Y_aux = sklearn.model_selection.train_test_split(X_data, Y_data, stratify = Y_data, test_size=0.66, random_state=42)\n    X_model_2, X_model_3, Y_model_2, Y_model_3 = sklearn.model_selection.train_test_split(X_aux, Y_aux, stratify = Y_aux, test_size=0.5, random_state=42)\n    return X_test, Y_test, X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3\n    \n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preparation(X_data, Y_data, new_h, new_w, channels):\n    x_train,x_val,y_train,y_val = train_test_split(X_data,Y_data,stratify=Y_data, train_size=0.8, random_state=42)\n    x_train = np.array(x_train) / 255\n    x_val = np.array(x_val) / 255\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    x_train = x_train.reshape(-1, new_h, new_w, channels)\n    x_val = x_val.reshape(-1, new_h, new_w, channels)\n    y_train = np.concatenate(y_train, axis=0)\n    y_val = np.concatenate(y_val, axis=0)\n    return x_train, x_val, y_train, y_val","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode target variables\ndef encode_target_variable(y_train,y_val):\n\n    class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n    d_class_weights = dict(enumerate(class_weights))\n    print(d_class_weights)\n    onehot_encoder = OneHotEncoder(sparse=False)\n    y_train = y_train.reshape(-1,1)\n    y_val = y_val.reshape(-1,1)\n    y_train_onehot = onehot_encoder.fit_transform(y_train)\n    y_val_onehot = onehot_encoder.fit_transform(y_val)\n    return y_train_onehot, y_val_onehot, d_class_weights","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_0(input_shape):\n    input_img = Input(shape=input_shape)  \n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(input_img)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(x)\n    x = Dropout(0.3)(x)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    y = MaxPooling2D((2, 2), padding='same')(y)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(y)\n    y = MaxPooling2D((2, 2), padding='same')(y)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(y)\n    y = Dropout(0.4)(y)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    z = MaxPooling2D((2, 2), padding='same')(z)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(z)\n    z = MaxPooling2D((2, 2), padding='same')(z)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(z)\n\n    out = layers.concatenate([x, y, z])\n    out = layers.Flatten()(out)\n    out = Dense(16, activation='selu')(out)\n    out = Dense(5, activation='softmax')(out)\n    \n    model_f = Model(inputs=[input_img], outputs=[out])\n    model_f.summary()\n    return model_f","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_1(x_train,x_val,input_shape):\n    weights_path = '../input/weightsclimbers/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    resnet50 = ResNet50(weights = None ,include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y,3))\n    resnet50.load_weights(weights_path, by_name=True)\n    resnet50.trainable = False # remove if you want to retrain resnet weights\n    # resnet50.summary()\n    transfer_model_1 = Sequential() \n    transfer_model_1.add(resnet50)\n    transfer_model_1.add(Flatten())\n    transfer_model_1.add(Dense(128, activation='relu'))\n    transfer_model_1.add(Dropout(0.2))\n    transfer_model_1.add(Dense(5, activation='softmax'))\n    transfer_model_1.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.resnet50.preprocess_input(x_train_new)\n    x_val_new = keras.applications.resnet50.preprocess_input(x_val_new)\n    return transfer_model_1, x_train_new, x_val_new","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_2(x_train,x_val):\n    weights_path = '../input/weightsclimbers/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    vgg16 = VGG16(weights=None, include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y, 3))\n    vgg16.load_weights(weights_path, by_name=True)\n    vgg16.trainable = False # remove if you want to retrain vgg16 weights\n    # vgg16.summary()\n    transfer_model_2 = Sequential()\n    transfer_model_2.add(vgg16)\n    transfer_model_2.add(Flatten())\n    transfer_model_2.add(Dense(128, activation='relu'))\n    transfer_model_2.add(Dropout(0.2))\n    transfer_model_2.add(Dense(5, activation='softmax'))\n    transfer_model_2.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.vgg16.preprocess_input(x_train_new)\n    x_val_new = keras.applications.vgg16.preprocess_input(x_val_new)\n    return transfer_model_2, x_train_new, x_val_new","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_3(x_train,x_val):\n    weights_path = '../input/weightsclimbers/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    inceptionV3 = InceptionV3(weights=None, include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y, 3))\n    inceptionV3.load_weights(weights_path, by_name=True)\n    inceptionV3.trainable = False # remove if you want to retrain rinceptionV3 weights\n    # inceptionV3.summary()\n    transfer_model_3 = Sequential()\n    transfer_model_3.add(inceptionV3)\n    transfer_model_3.add(Flatten())\n    transfer_model_3.add(Dense(128, activation='relu'))\n    transfer_model_3.add(Dropout(0.2))\n    transfer_model_3.add(Dense(5, activation='softmax'))\n    transfer_model_3.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.inception_v3.preprocess_input(x_train_new)\n    x_val_new = keras.applications.inception_v3.preprocess_input(x_val_new)\n    return transfer_model_3, x_train_new, x_val_new","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot,class_weights):\n    \n    batch_size = batch_size\n    epochs = epochs\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n              metrics=['categorical_accuracy',keras.metrics.AUC(),keras.metrics.Recall()])\n    \n    \n    history = model.fit(x_train_new, y_train_onehot, \\\n                              batch_size=batch_size, epochs=epochs, \\\n                              validation_split=0.2, verbose=1, shuffle=True, validation_data=(x_val_new, y_val_onehot),\n                       class_weight = class_weights)\n    return model","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Main function ######################\n\n\nnew_h = IMG_SIZE_X\nnew_w = IMG_SIZE_Y\nbatch_size = 500\nepochs = 10\nchannels = 3\n#Divide data into test and training/validation for three different models\nX_test, Y_test, X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3 = train_test_split_modified(data)\ntraining_data = [X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3]\n#Check that the training datasets are correctly stratified \nY_model_1_df = DataFrame(Y_model_1,columns=['labels'])\n#print(Y_model_1_df.labels.value_counts())\nY_model_2_df = DataFrame(Y_model_2,columns=['labels'])\n#print(Y_model_2_df.labels.value_counts())\nY_model_3_df = DataFrame(Y_model_3,columns=['labels'])\n#print(Y_model_3_df.labels.value_counts())\n#deaseases explanation: {\"0\": \"Cassava Bacterial Blight (CBB)\", \n#\"1\": \"Cassava Brown Streak Disease (CBSD)\", \"2\": \"Cassava Green Mottle (CGM)\", \n#\"3\": \"Cassava Mosaic Disease (CMD)\", \"4\": \"Healthy\"}\n\n# #visualize each of the classes\n# fig = plt.figure(figsize=(10, 6))\n\n# for i in range(8):\n#     img = X_model_1[i]\n#     fig.add_subplot(2, 4, i+1)\n#     plt.imshow(img)\n#     plt.title(Y_model_1[i])\n    \n#Baseline model\nx_train, x_val, y_train, y_val = data_preparation(training_data[0], training_data[1], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ninput_shape = [new_h,new_w,channels]\ndel y_train, y_val\nbaseline_model = create_model_0(input_shape)\ngc.collect()\nbaseline_model = train_model(baseline_model, batch_size, epochs, x_train, x_val, y_train_onehot, y_val_onehot, class_weights)\n#Resnet50\nx_train, x_val, y_train, y_val = data_preparation(training_data[0], training_data[1], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\nresnet50_model, x_train_new, x_val_new = create_model_1(x_train,x_val,input_shape)\ndel x_train,x_val\ngc.collect()\nresnet50_model = train_model(resnet50_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)\n#vgg16\nx_train, x_val, y_train, y_val = data_preparation(training_data[2], training_data[3], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\nvgg16_model, x_train_new, x_val_new = create_model_2(x_train,x_val)\ndel x_train,x_val\ngc.collect()\nvgg16_model = train_model(vgg16_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot)\n#InceptionV3\nx_train, x_val, y_train, y_val = data_preparation(training_data[4], training_data[5], new_h, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\ninception_model, x_train_new, x_val_new = create_model_3(x_train,x_val)\ndel x_train,x_val\ngc.collect()\ninception_model = train_model(inception_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)","execution_count":36,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[2 1 3 ... 3 1 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.937593984962406, 1: 1.9541044776119403, 2: 1.7934931506849314, 3: 0.32517851598882336, 4: 1.6625396825396825}\nModel: \"functional_19\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_18 (InputLayer)           [(None, 128, 128, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_81 (Conv2D)              (None, 128, 128, 16) 1216        input_18[0][0]                   \n__________________________________________________________________________________________________\nconv2d_84 (Conv2D)              (None, 128, 128, 16) 448         input_18[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_54 (MaxPooling2D) (None, 64, 64, 16)   0           conv2d_81[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_56 (MaxPooling2D) (None, 64, 64, 16)   0           conv2d_84[0][0]                  \n__________________________________________________________________________________________________\nconv2d_87 (Conv2D)              (None, 128, 128, 16) 448         input_18[0][0]                   \n__________________________________________________________________________________________________\nconv2d_82 (Conv2D)              (None, 64, 64, 16)   6416        max_pooling2d_54[0][0]           \n__________________________________________________________________________________________________\nconv2d_85 (Conv2D)              (None, 64, 64, 16)   2320        max_pooling2d_56[0][0]           \n__________________________________________________________________________________________________\nmax_pooling2d_58 (MaxPooling2D) (None, 64, 64, 16)   0           conv2d_87[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_55 (MaxPooling2D) (None, 32, 32, 16)   0           conv2d_82[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_57 (MaxPooling2D) (None, 32, 32, 16)   0           conv2d_85[0][0]                  \n__________________________________________________________________________________________________\nconv2d_88 (Conv2D)              (None, 64, 64, 16)   2320        max_pooling2d_58[0][0]           \n__________________________________________________________________________________________________\nconv2d_83 (Conv2D)              (None, 32, 32, 16)   6416        max_pooling2d_55[0][0]           \n__________________________________________________________________________________________________\nconv2d_86 (Conv2D)              (None, 32, 32, 16)   2320        max_pooling2d_57[0][0]           \n__________________________________________________________________________________________________\nmax_pooling2d_59 (MaxPooling2D) (None, 32, 32, 16)   0           conv2d_88[0][0]                  \n__________________________________________________________________________________________________\ndropout_18 (Dropout)            (None, 32, 32, 16)   0           conv2d_83[0][0]                  \n__________________________________________________________________________________________________\ndropout_19 (Dropout)            (None, 32, 32, 16)   0           conv2d_86[0][0]                  \n__________________________________________________________________________________________________\nconv2d_89 (Conv2D)              (None, 32, 32, 16)   2320        max_pooling2d_59[0][0]           \n__________________________________________________________________________________________________\nconcatenate_9 (Concatenate)     (None, 32, 32, 48)   0           dropout_18[0][0]                 \n                                                                 dropout_19[0][0]                 \n                                                                 conv2d_89[0][0]                  \n__________________________________________________________________________________________________\nflatten_9 (Flatten)             (None, 49152)        0           concatenate_9[0][0]              \n__________________________________________________________________________________________________\ndense_18 (Dense)                (None, 16)           786448      flatten_9[0][0]                  \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 5)            85          dense_18[0][0]                   \n==================================================================================================\nTotal params: 810,757\nTrainable params: 810,757\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 2s 272ms/step - loss: 7.2585 - categorical_accuracy: 0.4273 - auc_9: 0.6916 - recall_9: 0.3228 - val_loss: 1.4013 - val_categorical_accuracy: 0.6069 - val_auc_9: 0.7846 - val_recall_9: 0.6069\nEpoch 2/10\n9/9 [==============================] - 2s 197ms/step - loss: 1.3210 - categorical_accuracy: 0.5729 - auc_9: 0.7845 - recall_9: 0.5058 - val_loss: 1.2395 - val_categorical_accuracy: 0.6069 - val_auc_9: 0.7988 - val_recall_9: 0.6069\nEpoch 3/10\n9/9 [==============================] - 2s 188ms/step - loss: 1.2010 - categorical_accuracy: 0.6171 - auc_9: 0.7953 - recall_9: 0.4187 - val_loss: 1.2044 - val_categorical_accuracy: 0.6069 - val_auc_9: 0.7936 - val_recall_9: 0.5315\nEpoch 4/10\n9/9 [==============================] - 2s 190ms/step - loss: 1.1928 - categorical_accuracy: 0.6171 - auc_9: 0.8003 - recall_9: 0.4034 - val_loss: 1.2670 - val_categorical_accuracy: 0.6069 - val_auc_9: 0.8024 - val_recall_9: 0.6069\nEpoch 5/10\n9/9 [==============================] - 2s 189ms/step - loss: 1.1944 - categorical_accuracy: 0.6171 - auc_9: 0.8048 - recall_9: 0.3672 - val_loss: 1.1689 - val_categorical_accuracy: 0.6069 - val_auc_9: 0.8111 - val_recall_9: 0.6021\nEpoch 6/10\n9/9 [==============================] - 2s 190ms/step - loss: 1.6361 - categorical_accuracy: 0.4973 - auc_9: 0.7108 - recall_9: 0.3946 - val_loss: 1.1658 - val_categorical_accuracy: 0.6069 - val_auc_9: 0.8246 - val_recall_9: 0.6069\nEpoch 7/10\n9/9 [==============================] - 2s 184ms/step - loss: 1.0759 - categorical_accuracy: 0.6197 - auc_9: 0.8580 - recall_9: 0.5877 - val_loss: 1.2126 - val_categorical_accuracy: 0.6164 - val_auc_9: 0.7896 - val_recall_9: 0.4094\nEpoch 8/10\n9/9 [==============================] - 2s 188ms/step - loss: 1.1374 - categorical_accuracy: 0.6049 - auc_9: 0.8303 - recall_9: 0.4794 - val_loss: 1.1351 - val_categorical_accuracy: 0.6107 - val_auc_9: 0.8277 - val_recall_9: 0.5477\nEpoch 9/10\n9/9 [==============================] - 2s 199ms/step - loss: 1.0424 - categorical_accuracy: 0.6262 - auc_9: 0.8644 - recall_9: 0.5309 - val_loss: 1.1640 - val_categorical_accuracy: 0.6145 - val_auc_9: 0.8209 - val_recall_9: 0.3798\nEpoch 10/10\n9/9 [==============================] - 2s 187ms/step - loss: 1.0691 - categorical_accuracy: 0.6073 - auc_9: 0.8541 - recall_9: 0.4739 - val_loss: 1.1890 - val_categorical_accuracy: 0.6069 - val_auc_9: 0.8201 - val_recall_9: 0.5973\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[2 1 3 ... 3 1 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.937593984962406, 1: 1.9541044776119403, 2: 1.7934931506849314, 3: 0.32517851598882336, 4: 1.6625396825396825}\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet50 (Functional)        (None, 4, 4, 2048)        23587712  \n_________________________________________________________________\nflatten_10 (Flatten)         (None, 32768)             0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 128)               4194432   \n_________________________________________________________________\ndropout_20 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_21 (Dense)             (None, 5)                 645       \n=================================================================\nTotal params: 27,782,789\nTrainable params: 4,195,077\nNon-trainable params: 23,587,712\n_________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 5s 537ms/step - loss: 357.6570 - categorical_accuracy: 0.2535 - auc_10: 0.4919 - recall_10: 0.1494 - val_loss: 1.6058 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7543 - val_recall_10: 0.0000e+00\nEpoch 2/10\n9/9 [==============================] - 3s 307ms/step - loss: 1.6034 - categorical_accuracy: 0.6171 - auc_10: 0.7607 - recall_10: 0.0000e+00 - val_loss: 1.6003 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7543 - val_recall_10: 0.0000e+00\nEpoch 3/10\n9/9 [==============================] - 3s 298ms/step - loss: 1.5975 - categorical_accuracy: 0.6171 - auc_10: 0.7607 - recall_10: 0.0000e+00 - val_loss: 1.5941 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7543 - val_recall_10: 0.0000e+00\nEpoch 4/10\n9/9 [==============================] - 3s 302ms/step - loss: 1.5910 - categorical_accuracy: 0.6171 - auc_10: 0.7607 - recall_10: 0.0000e+00 - val_loss: 1.5876 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7543 - val_recall_10: 0.0000e+00\nEpoch 5/10\n9/9 [==============================] - 3s 306ms/step - loss: 1.5841 - categorical_accuracy: 0.6171 - auc_10: 0.7607 - recall_10: 0.0000e+00 - val_loss: 1.5808 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7543 - val_recall_10: 0.0000e+00\nEpoch 6/10\n9/9 [==============================] - 3s 340ms/step - loss: 1.5772 - categorical_accuracy: 0.6171 - auc_10: 0.7746 - recall_10: 0.0000e+00 - val_loss: 1.5740 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7719 - val_recall_10: 0.0000e+00\nEpoch 7/10\n9/9 [==============================] - 3s 300ms/step - loss: 1.5701 - categorical_accuracy: 0.6171 - auc_10: 0.7840 - recall_10: 0.0000e+00 - val_loss: 1.5670 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7629 - val_recall_10: 0.0000e+00\nEpoch 8/10\n9/9 [==============================] - 3s 295ms/step - loss: 1.5630 - categorical_accuracy: 0.6171 - auc_10: 0.7677 - recall_10: 0.0000e+00 - val_loss: 1.5602 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7543 - val_recall_10: 0.0000e+00\nEpoch 9/10\n9/9 [==============================] - 3s 300ms/step - loss: 1.5560 - categorical_accuracy: 0.6171 - auc_10: 0.7599 - recall_10: 0.0000e+00 - val_loss: 1.5534 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7543 - val_recall_10: 0.0000e+00\nEpoch 10/10\n9/9 [==============================] - 3s 299ms/step - loss: 1.5490 - categorical_accuracy: 0.6171 - auc_10: 0.7613 - recall_10: 0.0000e+00 - val_loss: 1.5465 - val_categorical_accuracy: 0.6069 - val_auc_10: 0.7543 - val_recall_10: 0.0000e+00\n{0: 3.9410852713178293, 1: 1.9553846153846153, 2: 1.7932980599647266, 3: 0.3252719129878439, 4: 1.6587275693311583}\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[3 2 2 ... 4 1 2] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-26e607ffd9e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#vgg16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0my_train_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_target_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mvgg16_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef load_test_data(new_w,new_h):\n    path = '../input/cassava-leaf-disease-classification/test_images'\n    path_loop = r'../input/cassava-leaf-disease-classification/test_images/*.*'\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data = []\n    name_files = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        x_train = np.array(frame_rgb) / 255\n        x_train = x_train.reshape(-1, new_h, new_w, channels)\n        data.append([x_train])\n        name_files.append(name_file)\n    return data, name_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the CSV\ndata, name_files = load_test_data(IMG_SIZE_X,IMG_SIZE_Y)\npredictions = inception_model.predict(data)\npred_df = pd.DataFrame(predictions)\npred_list = pred_df.idxmax(axis=\"columns\")\npred_df = pd.DataFrame(zip(name_files, pred_list), columns = [\"image_id\", \"label\"])\npred_df.head()\npd.DataFrame(pred_df, columns=['predictions']).to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}