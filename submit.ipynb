{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pylab as plt\nimport glob\nimport os\nimport random\nimport keras\nfrom tensorflow.keras.utils import plot_model\nfrom keras import backend as K\nfrom keras import layers\nfrom keras import metrics\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, GlobalAveragePooling2D\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.models import Sequential, Model\nfrom keras.utils import plot_model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport cv2\nfrom tqdm import tqdm\nfrom keras.callbacks import EarlyStopping\n# from keras.applications.resnet50 import ResNet50, preprocess_input\n# from tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.applications import EfficientNetB2\n# from keras.applications.vgg16 import VGG16, preprocess_input\n# from keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom collections import Counter\nfrom pandas import DataFrame\nimport gc\nimport tensorflow as tf\nfrom sklearn.utils import class_weight\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.utils import resample","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# efficientNetB2 = EfficientNetB2(weights = None ,include_top=False, input_shape=input_shape)\n# len(efficientNetB2.layers)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode target variables\ndef encode_target_variable(y_train,y_val):\n\n    class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n    d_class_weights = dict(enumerate(class_weights))\n    onehot_encoder = OneHotEncoder(sparse=False)\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    y_train = y_train.reshape(-1,1)\n    y_val = y_val.reshape(-1,1)\n    y_train_onehot = onehot_encoder.fit_transform(y_train)\n    y_val_onehot = onehot_encoder.fit_transform(y_val)\n    return y_train_onehot, y_val_onehot, d_class_weights","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv_path = \"../input/cassava-leaf-disease-classification/train.csv\"\nlabel_json_path = \"../input/cassava-leaf-disease-classification/label_num_to_disease_map.json\"\nimages_dir_path = \"../input/cassava-leaf-disease-classification/train_images\"\ntest_csv_path = \"../input/cassava-leaf-disease-classification/sample_submission.csv\"\ntrain_csv = pd.read_csv(train_csv_path)\ntrain_csv['label'] = train_csv['label'].astype('string')\n#Undersample\n# x_undersampled, y_undersampled = resample(train_csv.loc[train_csv['label'] == 3,'image_id'], train_csv.loc[train_csv['label'] == 3,'label'],\n#                  replace=True,\n#                  n_samples= 3000,\n#                  random_state=123)\n\n# undersample_df = pd.DataFrame(columns=['image_id','label'])\n# undersample_df['image_id'] = x_undersampled\n# undersample_df['label'] = y_undersampled\n\n# train_df = undersample_df.append([train_csv.loc[train_csv['label'] == 0,:],train_csv.loc[train_csv['label'] == 1,:],train_csv.loc[train_csv['label'] == 2,:],train_csv.loc[train_csv['label'] == 4,:]])\n# train_df['label'] = train_df['label'].astype('string')\n\nx_train, x_val, y_train, y_val = train_test_split(train_csv['image_id'], train_csv['label'], test_size = 0.05, random_state = 27, stratify=train_csv['label'])\n\ny_train_onehot, y_val_onehot, d_class_weights = encode_target_variable(y_train,y_val)\n\n\n#train df\ndf_train = pd.DataFrame(columns=['image_id','label'])\ndf_train['image_id'] = x_train\ndf_train['label'] = y_train\n#validation df\ndf_val = pd.DataFrame(columns=['image_id','label'])\ndf_val['image_id'] = x_val\ndf_val['label'] = y_val\n\ndf_train.reset_index(drop=True, inplace=True)\ndf_val.reset_index(drop=True, inplace=True)\n\nlabel_class = pd.read_json(label_json_path, orient='index')\nlabel_class = label_class.values.flatten().tolist()\nIMG_SIZE = 600\nBATCH_SIZE = 32\nEPOCHS = 4\nCHANNELS = 3","execution_count":4,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['0' '1' '2' '3' '4'], y=5020     3\n18807    1\n8537     3\n16442    3\n15754    4\n        ..\n13788    1\n18853    3\n4310     1\n17081    2\n4939     3\nName: label, Length: 20327, dtype: string as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Label names :\")\nfor i, label in enumerate(label_class):\n    print(f\" {i}. {label}\")","execution_count":5,"outputs":[{"output_type":"stream","text":"Label names :\n 0. Cassava Bacterial Blight (CBB)\n 1. Cassava Brown Streak Disease (CBSD)\n 2. Cassava Green Mottle (CGM)\n 3. Cassava Mosaic Disease (CMD)\n 4. Healthy\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bi-tempered log loss for noisy labels datasets\n\n# Tempered Softmax Activation\n\ndef log_t(u, t):\n    epsilon = 1e-7\n    \"\"\"Compute log_t for `u`.\"\"\"\n    if t == 1.0:\n        return tf.math.log(u + epsilon)\n    else:\n        return (u**(1.0 - t) - 1.0) / (1.0 - t)\n\ndef exp_t(u, t):\n    \"\"\"Compute exp_t for `u`.\"\"\"\n    if t == 1.0:\n        return tf.math.exp(u)\n    else:\n        return tf.math.maximum(0.0, 1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))\n\ndef compute_normalization_fixed_point(y_pred, t2, num_iters=5):\n    \"\"\"Returns the normalization value for each example (t > 1.0).\n    Args:\n    y_pred: A multi-dimensional tensor with last dimension `num_classes`.\n    t2: A temperature 2 (> 1.0 for tail heaviness).\n    num_iters: Number of iterations to run the method.\n    Return: A tensor of same rank as y_pred with the last dimension being 1.\n    \"\"\"\n    mu = tf.math.reduce_max(y_pred, -1, keepdims=True)\n    normalized_y_pred_step_0 = y_pred - mu\n    normalized_y_pred = normalized_y_pred_step_0\n    i = 0\n    while i < num_iters:\n        i += 1\n        logt_partition = tf.math.reduce_sum(exp_t(normalized_y_pred, t2),-1, keepdims=True)\n        normalized_y_pred = normalized_y_pred_step_0 * (logt_partition ** (1.0 - t2))\n  \n    logt_partition = tf.math.reduce_sum(exp_t(normalized_y_pred, t2), -1, keepdims=True)\n    return -log_t(1.0 / logt_partition, t2) + mu\n\ndef compute_normalization(y_pred, t2, num_iters=5):\n    \"\"\"Returns the normalization value for each example.\n    Args:\n    y_pred: A multi-dimensional tensor with last dimension `num_classes`.\n    t2: A temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n    num_iters: Number of iterations to run the method.\n    Return: A tensor of same rank as activation with the last dimension being 1.\n    \"\"\"\n    if t2 < 1.0:\n        return None # not implemented as these values do not occur in the authors experiments...\n    else:\n        return compute_normalization_fixed_point(y_pred, t2, num_iters)\n\ndef tempered_softmax_activation(x, t2=1., num_iters=5):\n    \"\"\"Tempered softmax function.\n    Args:\n    x: A multi-dimensional tensor with last dimension `num_classes`.\n    t2: A temperature tensor > 0.0.\n    num_iters: Number of iterations to run the method.\n    Returns:\n    A probabilities tensor.\n    \"\"\"\n    if t2 == 1.0:\n        normalization_constants = tf.math.log(tf.math.reduce_sum(tf.math.exp(x), -1, keepdims=True))\n    else:\n        normalization_constants = compute_normalization(x, t2, num_iters)\n\n    return exp_t(x - normalization_constants, t2)\n\nclass TemperedSoftmax(tf.keras.layers.Layer):\n    def __init__(self, t2=1.0, num_iters=5, **kwargs):\n        super(TemperedSoftmax, self).__init__(**kwargs)\n        self.t2 = t2\n        self.num_iters = num_iters\n\n    def call(self, inputs):\n        return tempered_softmax_activation(inputs, t2=self.t2, num_iters=self.num_iters)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bi_tempered_logistic_loss(y_pred, y_true, t1, label_smoothing=0.0):\n    \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n    Args:\n    y_pred: A multi-dimensional probability tensor with last dimension `num_classes`.\n    y_true: A tensor with shape and dtype as y_pred.\n    t1: Temperature 1 (< 1.0 for boundedness).\n    label_smoothing: A float in [0, 1] for label smoothing.\n    Returns:\n    A loss tensor.\n    \"\"\"\n    y_pred = tf.cast(y_pred, tf.float32)\n    y_true = tf.cast(y_true, tf.float32)\n\n    if label_smoothing > 0.0:\n        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n        y_true = (1 - num_classes /(num_classes - 1) * label_smoothing) * y_true + label_smoothing / (num_classes - 1)\n\n    temp1 = (log_t(y_true + 1e-7, t1) - log_t(y_pred, t1)) * y_true\n    temp2 = (1 / (2 - t1)) * (tf.math.pow(y_true, 2 - t1) - tf.math.pow(y_pred, 2 - t1))\n    loss_values = temp1 - temp2\n\n    return tf.math.reduce_sum(loss_values, -1)\n\nclass BiTemperedLogisticLoss(tf.keras.losses.Loss):\n    def __init__(self, t1, label_smoothing=0.0):\n        super(BiTemperedLogisticLoss, self).__init__()\n        self.t1 = t1\n        self.label_smoothing = label_smoothing\n\n    def call(self, y_true, y_pred):\n        return bi_tempered_logistic_loss(y_pred, y_true, self.t1, self.label_smoothing)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = ImageDataGenerator(\n                    preprocessing_function = tf.keras.applications.efficientnet.preprocess_input,\n                    rotation_range = 180,\n                    width_shift_range = 0.2,\n                    height_shift_range = 0.2,\n                    shear_range = 25,\n                    zoom_range = 0.5,\n                    horizontal_flip = True,\n                    vertical_flip = True,\n                    channel_shift_range=15.0,\n                    brightness_range=(0.4, 0.6),\n                    fill_mode = 'reflect'\n                               )\n\n\n                                    \n    \nvalid_gen = ImageDataGenerator(tf.keras.applications.efficientnet.preprocess_input,\n                               validation_split = 0.2\n                              )\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = train_gen.flow_from_dataframe(\n                            dataframe=df_train,\n                            directory = images_dir_path,\n                            x_col = \"image_id\",\n                            y_col = \"label\",\n                            target_size = (IMG_SIZE, IMG_SIZE),\n                            class_mode = \"categorical\",\n                            shuffle = True,\n                            subset = \"training\"\n\n)\n\nvalid_generator = valid_gen.flow_from_dataframe(\n                            dataframe=df_val,\n                            directory = images_dir_path,\n                            x_col = \"image_id\",\n                            y_col = \"label\",\n                            target_size = (IMG_SIZE, IMG_SIZE),\n                            class_mode = \"categorical\",\n                            batch_size = BATCH_SIZE,\n                            shuffle = False,\n                            subset = \"validation\"\n)\n","execution_count":9,"outputs":[{"output_type":"stream","text":"Found 20327 validated image filenames belonging to 5 classes.\nFound 214 validated image filenames belonging to 5 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_0(input_shape):\n    input_img = Input(shape=input_shape)  \n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(input_img)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(x)\n    x = Dropout(0.3)(x)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    y = MaxPooling2D((2, 2), padding='same')(y)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(y)\n    y = MaxPooling2D((2, 2), padding='same')(y)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(y)\n    y = Dropout(0.4)(y)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    z = MaxPooling2D((2, 2), padding='same')(z)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(z)\n    z = MaxPooling2D((2, 2), padding='same')(z)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(z)\n\n    #out = layers.concatenate([x, y, z])\n    out = layers.Flatten()(z)\n    out = Dense(16, activation='relu')(out)\n    out = Dense(5, activation='softmax')(out)\n    \n    model_f = Model(inputs=[input_img], outputs=[out])\n    model_f.summary()\n    return model_f","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# efficientNetB3 = EfficientNetB3(weights = None ,include_top=False, input_shape=input_shape)\n# print(\"Number of layers in the base model: \", len(efficientNetB3.layers))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_1(input_shape):\n    weights_path = '../input/effib3-vasiliy/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5'\n    efficientNetB3 = EfficientNetB3(weights = None ,include_top=False, input_shape=input_shape)\n    efficientNetB3.load_weights(weights_path, by_name=True)\n    efficientNetB3.trainable = True\n\n    # Let's take a look to see how many layers are in the base model\n    print(\"Number of layers in the base model: \", len(efficientNetB3.layers))\n\n    # Fine-tune from this layer onwards\n    fine_tune_at = 334\n\n    # Freeze all the layers before the `fine_tune_at` layer\n    for layer in efficientNetB3.layers[:fine_tune_at]:\n          layer.trainable =  False\n\n     # remove if you want to retrain resnet weights\n    tempered_softmax_2 = TemperedSoftmax(t2=2)\n    # resnet50.summary()\n    model = Sequential([\n    efficientNetB3,\n    GlobalAveragePooling2D(),\n    Flatten(),\n    Dense(256, activation = 'relu', bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001)),\n    Dropout(0.5),\n    Dense(5, activation='softmax')\n    #tempered_softmax_2\n    ])\n    print(model.summary())\n    \n    return model","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_2(input_shape):\n    weights_path = '../input/weightsclimbers/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    vgg16 = VGG16(weights=None, include_top=False, input_shape=input_shape)\n    vgg16.load_weights(weights_path, by_name=True)\n    vgg16.trainable = False # remove if you want to retrain vgg16 weights\n    # vgg16.summary()\n    transfer_model_2 = Sequential()\n    transfer_model_2.add(vgg16)\n    transfer_model_2.add(Flatten())\n    transfer_model_2.add(Dense(128, activation='relu'))\n    transfer_model_2.add(Dropout(0.2))\n    transfer_model_2.add(Dense(5, activation='softmax'))\n    transfer_model_2.summary()\n\n    return transfer_model_2","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_3(x_train,x_val):\n    weights_path = '../input/weightsclimbers/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    inceptionV3 = InceptionV3(weights=None, include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y, 3))\n    inceptionV3.load_weights(weights_path, by_name=False)\n    inceptionV3.trainable = False # remove if you want to retrain rinceptionV3 weights\n    # inceptionV3.summary()\n    transfer_model_3 = Sequential()\n    transfer_model_3.add(inceptionV3)\n    transfer_model_3.add(Flatten())\n    transfer_model_3.add(Dense(128, activation='relu'))\n    transfer_model_3.add(Dropout(0.2))\n    transfer_model_3.add(Dense(5, activation='softmax'))\n    transfer_model_3.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.inception_v3.preprocess_input(x_train_new)\n    x_val_new = keras.applications.inception_v3.preprocess_input(x_val_new)\n    return transfer_model_3, x_train_new, x_val_new","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, batch_size, epochs):\n    \n    batch_size = batch_size\n    epochs = epochs\n    \n    def scheduler(epoch, lr):\n        if epoch >2 and epoch%2==0:\n            return lr/1.25\n        else:\n            return lr\n        \n    btll_02 = BiTemperedLogisticLoss(t1=0.8)\n    \n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits = False,\n                                                   label_smoothing=0.0001,\n                                                   name='categorical_crossentropy' )\n    \n    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001), \n              metrics=[\"categorical_accuracy\",keras.metrics.AUC(),keras.metrics.Recall()])\n    \n    earlyStop_callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", \n                                                          patience=2, restore_best_weights = True, min_delta = 0.01)\n    \n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, \n                              patience = 2, min_delta = 0.01, \n                              mode = 'min', verbose = 1)\n    \n    \n    history = model.fit_generator(train_generator, \n                    epochs=epochs,  # one forward/backward pass of training data\n                    steps_per_epoch=train_generator.samples//train_generator.batch_size,  # number of images comprising of one epoch\n                    validation_data=valid_generator, # Or validation_data=valid_generator\n                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n                    callbacks = [earlyStop_callback, reduce_lr])\n\n    return model, history","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Main function ######################\n\n    \n#Baseline model\n# y_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ninput_shape = [IMG_SIZE,IMG_SIZE,CHANNELS]\n#baseline_model = create_model_0(input_shape)\n#baseline_model = train_model(baseline_model, BATCH_SIZE, EPOCHS,class_weights)\n\n\n# #Resnet50\n# EfficientNet_model = create_model_1(input_shape)\n# EfficientNet_model, history = train_model(EfficientNet_model, BATCH_SIZE, EPOCHS)\n# #vgg16\n# x_train, x_val, y_train, y_val = data_preparation(training_data[2], training_data[3], new_h, new_w, channels)\n# y_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\n# del y_train, y_val\n#vgg16_model = create_model_2(input_shape)\n#vgg16_model = train_model(vgg16_model, BATCH_SIZE, EPOCHS, class_weights)\n# #InceptionV3\n# x_train, x_val, y_train, y_val = data_preparation(training_data[4], training_data[5], new_h,new_w, channels)\n# y_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\n# del y_train, y_val\n# inception_model, x_train_new, x_val_new = create_model_3(x_train,x_val)\n# del x_train,x_val\n# gc.collect()\n# inception_model = train_model(inception_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras.models.save_model(EfficientNet_model,'efinetb3_vasily_0layers.h5')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EfficientNet_model = keras.models.load_model('../input/trained-effib3-vasily-50lay/efinetb3_vasily_50layers.h5')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nmodel = EfficientNet_model\n# model = model1\n# model = model2\nY_pred = model.predict_generator(valid_generator, valid_generator.samples // valid_generator.batch_size+1)\ny_pred = np.argmax(Y_pred, axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(valid_generator.classes, y_pred))\nprint('Classification Report')\ntarget_names = ['0', '1', '2', '3', '4']\nprint(classification_report(valid_generator.classes, y_pred, target_names=target_names))","execution_count":19,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n  warnings.warn('This ImageDataGenerator specifies '\n","name":"stderr"},{"output_type":"stream","text":"Confusion Matrix\n[[  9   1   0   2   1]\n [  0  19   1   4   1]\n [  0   2  19   6   2]\n [  0   1   0 127   0]\n [  0   1   1   4  13]]\nClassification Report\n              precision    recall  f1-score   support\n\n           0       1.00      0.69      0.82        13\n           1       0.79      0.76      0.78        25\n           2       0.90      0.66      0.76        29\n           3       0.89      0.99      0.94       128\n           4       0.76      0.68      0.72        19\n\n    accuracy                           0.87       214\n   macro avg       0.87      0.76      0.80       214\nweighted avg       0.87      0.87      0.87       214\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TTA_boolean = 1 #0\n\npreds = []\ndirec = \"../input/cassava-leaf-disease-classification/\"\ntest_direc = direc + \"test_images/\"\nsample_sub_csv = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n\nif TTA_boolean == 1:\n    test_gen = ImageDataGenerator(\n                        preprocessing_function = tf.keras.applications.efficientnet.preprocess_input,\n                        rotation_range=40,\n                         width_shift_range=[0.1,0.4],\n                         height_shift_range=[0.1,0.4],\n                         zoom_range=[0.2,0.4],\n                        fill_mode = 'reflect'\n                                   )\n\n    #fig = plt.figure(figsize=(15, 10))\n    for image in sample_sub_csv.image_id:\n        img = keras.preprocessing.image.load_img('../input/cassava-leaf-disease-classification/test_images/' + image)\n        img = keras.preprocessing.image.img_to_array(img)\n        img = keras.preprocessing.image.smart_resize(img, (IMG_SIZE, IMG_SIZE))\n        tta_steps = 10\n        predictions = []\n        for i in range(tta_steps):\n            img = tf.reshape(img, (-1, IMG_SIZE, IMG_SIZE, 3))\n            img_augmented = test_gen.flow(img,batch_size=1) \n            #fig.add_subplot(2, 5, i+1)\n            #print(np.squeeze(img_augmented[0]))\n            #plt.imshow(np.squeeze(img_augmented[0]).astype(int))\n            preds_tta = EfficientNet_model.predict_generator(img_augmented)\n            predictions.append(preds_tta)\n           \n\n        pred = np.mean(predictions, axis=0)\n\n        preds.append(np.argmax(pred))\n\nelse:\n    pass  \n        \nfinal_submission = pd.DataFrame({'image_id': sample_sub_csv.image_id, 'label': preds})\nfinal_submission.to_csv('submission.csv', index=False)","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x720 with 10 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}