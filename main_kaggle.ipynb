{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pylab as plt\nimport glob\nimport os\nimport random\nimport keras\nfrom keras import backend as K\nfrom keras import layers\nfrom keras import metrics\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.models import Sequential, Model\nfrom keras.utils import plot_model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport cv2\nfrom tqdm import tqdm\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom collections import Counter\nfrom pandas import DataFrame\nimport gc\nfrom sklearn.utils import class_weight\nfrom sklearn.utils.class_weight import compute_class_weight","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data_local(new_w,new_h):\n    train_labels = pd.read_csv(r'C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train.csv',sep=',')\n    path_loop = r\"C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train_images\\*.*\"\n    path = r\"C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train_images\"\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        label = train_labels.loc[train_labels['image_id'] == name_file,'label'].values\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        data.append([frame_rgb,label])\n    return data","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(new_w,new_h):\n    train_labels = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv', sep=',')\n    path = '../input/cassava-leaf-disease-classification/train_images'\n    path_loop = r'../input/cassava-leaf-disease-classification/train_images/*.*'\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        label = train_labels.loc[train_labels['image_id'] == name_file,'label'].values\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        data.append([frame_rgb,label])\n    return data\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change before submitting to Kaggle from load_data_local to load_data\nIMG_SIZE_X = 128\nIMG_SIZE_Y = 128\ndata = load_data(IMG_SIZE_X, IMG_SIZE_Y)","execution_count":6,"outputs":[{"output_type":"stream","text":"100%|██████████| 21397/21397 [06:49<00:00, 52.30it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split_modified(data):\n    x_data = []\n    y_data = []\n    \n    for feature, label in data:\n        x_data.append(feature)\n        y_data.append(label)\n        \n    X_data, X_test, Y_data, Y_test = sklearn.model_selection.train_test_split(x_data, y_data, stratify = y_data, test_size=0.1, random_state=42)\n    X_model_1, X_aux, Y_model_1, Y_aux = sklearn.model_selection.train_test_split(X_data, Y_data, stratify = Y_data, test_size=0.66, random_state=42)\n    X_model_2, X_model_3, Y_model_2, Y_model_3 = sklearn.model_selection.train_test_split(X_aux, Y_aux, stratify = Y_aux, test_size=0.5, random_state=42)\n    return X_test, Y_test, X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3\n    \n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preparation(X_data, Y_data, new_h, new_w, channels):\n    x_train,x_val,y_train,y_val = train_test_split(X_data,Y_data,stratify=Y_data, train_size=0.8, random_state=42)\n    x_train = np.array(x_train) / 255\n    x_val = np.array(x_val) / 255\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    x_train = x_train.reshape(-1, new_h, new_w, channels)\n    x_val = x_val.reshape(-1, new_h, new_w, channels)\n    y_train = np.concatenate(y_train, axis=0)\n    y_val = np.concatenate(y_val, axis=0)\n    return x_train, x_val, y_train, y_val","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode target variables\ndef encode_target_variable(y_train,y_val):\n\n    class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n    d_class_weights = dict(enumerate(class_weights))\n    print(d_class_weights)\n    onehot_encoder = OneHotEncoder(sparse=False)\n    y_train = y_train.reshape(-1,1)\n    y_val = y_val.reshape(-1,1)\n    y_train_onehot = onehot_encoder.fit_transform(y_train)\n    y_val_onehot = onehot_encoder.fit_transform(y_val)\n    return y_train_onehot, y_val_onehot, d_class_weights","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_0(input_shape):\n    input_img = Input(shape=input_shape)  \n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(input_img)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(x)\n    x = Dropout(0.3)(x)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    y = MaxPooling2D((2, 2), padding='same')(y)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(y)\n    y = MaxPooling2D((2, 2), padding='same')(y)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(y)\n    y = Dropout(0.4)(y)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    z = MaxPooling2D((2, 2), padding='same')(z)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(z)\n    z = MaxPooling2D((2, 2), padding='same')(z)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(z)\n\n    out = layers.concatenate([x, y, z])\n    out = layers.Flatten()(out)\n    out = Dense(16, activation='selu')(out)\n    out = Dense(5, activation='softmax')(out)\n    \n    model_f = Model(inputs=[input_img], outputs=[out])\n    model_f.summary()\n    return model_f","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_1(x_train,x_val,input_shape):\n    weights_path = '../input/weightsclimbers/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    resnet50 = ResNet50(weights = None ,include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y,3))\n    resnet50.load_weights(weights_path, by_name=True)\n    resnet50.trainable = False # remove if you want to retrain resnet weights\n    # resnet50.summary()\n    transfer_model_1 = Sequential() \n    transfer_model_1.add(resnet50)\n    transfer_model_1.add(Flatten())\n    transfer_model_1.add(Dense(128, activation='relu'))\n    transfer_model_1.add(Dropout(0.2))\n    transfer_model_1.add(Dense(5, activation='softmax'))\n    transfer_model_1.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.resnet50.preprocess_input(x_train_new)\n    x_val_new = keras.applications.resnet50.preprocess_input(x_val_new)\n    return transfer_model_1, x_train_new, x_val_new","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_2(x_train,x_val):\n    weights_path = '../input/weightsclimbers/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    vgg16 = VGG16(weights=None, include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y, 3))\n    vgg16.load_weights(weights_path, by_name=True)\n    vgg16.trainable = False # remove if you want to retrain vgg16 weights\n    # vgg16.summary()\n    transfer_model_2 = Sequential()\n    transfer_model_2.add(vgg16)\n    transfer_model_2.add(Flatten())\n    transfer_model_2.add(Dense(128, activation='relu'))\n    transfer_model_2.add(Dropout(0.2))\n    transfer_model_2.add(Dense(5, activation='softmax'))\n    transfer_model_2.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.vgg16.preprocess_input(x_train_new)\n    x_val_new = keras.applications.vgg16.preprocess_input(x_val_new)\n    return transfer_model_2, x_train_new, x_val_new","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_3(x_train,x_val):\n    weights_path = '../input/weightsclimbers/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    inceptionV3 = InceptionV3(weights=None, include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y, 3))\n    inceptionV3.load_weights(weights_path, by_name=False)\n    inceptionV3.trainable = False # remove if you want to retrain rinceptionV3 weights\n    # inceptionV3.summary()\n    transfer_model_3 = Sequential()\n    transfer_model_3.add(inceptionV3)\n    transfer_model_3.add(Flatten())\n    transfer_model_3.add(Dense(128, activation='relu'))\n    transfer_model_3.add(Dropout(0.2))\n    transfer_model_3.add(Dense(5, activation='softmax'))\n    transfer_model_3.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.inception_v3.preprocess_input(x_train_new)\n    x_val_new = keras.applications.inception_v3.preprocess_input(x_val_new)\n    return transfer_model_3, x_train_new, x_val_new","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot,class_weights):\n    \n    batch_size = batch_size\n    epochs = epochs\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n              metrics=['categorical_accuracy',keras.metrics.AUC(),keras.metrics.Recall()])\n    \n    \n    history = model.fit(x_train_new, y_train_onehot, \\\n                              batch_size=batch_size, epochs=epochs, \\\n                              validation_split=0.2, verbose=1, shuffle=True, validation_data=(x_val_new, y_val_onehot),\n                       class_weight = class_weights)\n    return model","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Main function ######################\n\n\nnew_h = IMG_SIZE_X\nnew_w = IMG_SIZE_Y\nbatch_size = 500\nepochs = 10\nchannels = 3\n#Divide data into test and training/validation for three different models\nX_test, Y_test, X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3 = train_test_split_modified(data)\ntraining_data = [X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3]\n#Check that the training datasets are correctly stratified \nY_model_1_df = DataFrame(Y_model_1,columns=['labels'])\n#print(Y_model_1_df.labels.value_counts())\nY_model_2_df = DataFrame(Y_model_2,columns=['labels'])\n#print(Y_model_2_df.labels.value_counts())\nY_model_3_df = DataFrame(Y_model_3,columns=['labels'])\n#print(Y_model_3_df.labels.value_counts())\n#deaseases explanation: {\"0\": \"Cassava Bacterial Blight (CBB)\", \n#\"1\": \"Cassava Brown Streak Disease (CBSD)\", \"2\": \"Cassava Green Mottle (CGM)\", \n#\"3\": \"Cassava Mosaic Disease (CMD)\", \"4\": \"Healthy\"}\n\n# #visualize each of the classes\n# fig = plt.figure(figsize=(10, 6))\n\n# for i in range(8):\n#     img = X_model_1[i]\n#     fig.add_subplot(2, 4, i+1)\n#     plt.imshow(img)\n#     plt.title(Y_model_1[i])\n    \n#Baseline model\nx_train, x_val, y_train, y_val = data_preparation(training_data[0], training_data[1], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ninput_shape = [new_h,new_w,channels]\ndel y_train, y_val\nbaseline_model = create_model_0(input_shape)\ngc.collect()\nbaseline_model = train_model(baseline_model, batch_size, epochs, x_train, x_val, y_train_onehot, y_val_onehot, class_weights)\n#Resnet50\nx_train, x_val, y_train, y_val = data_preparation(training_data[0], training_data[1], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\nresnet50_model, x_train_new, x_val_new = create_model_1(x_train,x_val,input_shape)\ndel x_train,x_val\ngc.collect()\nresnet50_model = train_model(resnet50_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)\n#vgg16\nx_train, x_val, y_train, y_val = data_preparation(training_data[2], training_data[3], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\nvgg16_model, x_train_new, x_val_new = create_model_2(x_train,x_val)\ndel x_train,x_val\ngc.collect()\nvgg16_model = train_model(vgg16_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)\n#InceptionV3\nx_train, x_val, y_train, y_val = data_preparation(training_data[4], training_data[5], new_h,new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\ninception_model, x_train_new, x_val_new = create_model_3(x_train,x_val)\ndel x_train,x_val\ngc.collect()\ninception_model = train_model(inception_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)","execution_count":15,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[2 1 3 ... 3 1 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.937593984962406, 1: 1.9541044776119403, 2: 1.7934931506849314, 3: 0.32517851598882336, 4: 1.6625396825396825}\nModel: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 128, 128, 16) 1216        input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 128, 128, 16) 448         input_1[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 16)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 128, 128, 16) 448         input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 64, 64, 16)   6416        max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 64, 64, 16)   2320        max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 16)   0           conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 16)   0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 64, 64, 16)   2320        max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 32, 32, 16)   6416        max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 16)   0           conv2d_7[0][0]                   \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 32, 32, 16)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 32, 32, 16)   0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 32, 32, 48)   0           dropout[0][0]                    \n                                                                 dropout_1[0][0]                  \n                                                                 conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 49152)        0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 16)           786448      flatten[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 5)            85          dense[0][0]                      \n==================================================================================================\nTotal params: 810,757\nTrainable params: 810,757\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 3s 323ms/step - loss: 3.2961 - categorical_accuracy: 0.1743 - auc: 0.5086 - recall: 0.0984 - val_loss: 1.5210 - val_categorical_accuracy: 0.1193 - val_auc: 0.6151 - val_recall: 0.0000e+00\nEpoch 2/10\n9/9 [==============================] - 2s 193ms/step - loss: 1.5967 - categorical_accuracy: 0.2392 - auc: 0.5984 - recall: 0.0060 - val_loss: 1.5270 - val_categorical_accuracy: 0.4857 - val_auc: 0.7342 - val_recall: 0.0000e+00\nEpoch 3/10\n9/9 [==============================] - 2s 194ms/step - loss: 1.5033 - categorical_accuracy: 0.2893 - auc: 0.6395 - recall: 0.0019 - val_loss: 1.8254 - val_categorical_accuracy: 0.1164 - val_auc: 0.5534 - val_recall: 0.0601\nEpoch 4/10\n9/9 [==============================] - 2s 197ms/step - loss: 1.5075 - categorical_accuracy: 0.2528 - auc: 0.6385 - recall: 0.0181 - val_loss: 1.3203 - val_categorical_accuracy: 0.5477 - val_auc: 0.7985 - val_recall: 0.0219\nEpoch 5/10\n9/9 [==============================] - 2s 193ms/step - loss: 1.3957 - categorical_accuracy: 0.3533 - auc: 0.6892 - recall: 0.0559 - val_loss: 1.3843 - val_categorical_accuracy: 0.5448 - val_auc: 0.7777 - val_recall: 0.0105\nEpoch 6/10\n9/9 [==============================] - 2s 198ms/step - loss: 1.3196 - categorical_accuracy: 0.4567 - auc: 0.7511 - recall: 0.0406 - val_loss: 1.2629 - val_categorical_accuracy: 0.6088 - val_auc: 0.8169 - val_recall: 0.0553\nEpoch 7/10\n9/9 [==============================] - 2s 189ms/step - loss: 1.3037 - categorical_accuracy: 0.4574 - auc: 0.7557 - recall: 0.0666 - val_loss: 1.4411 - val_categorical_accuracy: 0.3015 - val_auc: 0.7158 - val_recall: 0.0029\nEpoch 8/10\n9/9 [==============================] - 2s 203ms/step - loss: 1.2108 - categorical_accuracy: 0.4820 - auc: 0.8064 - recall: 0.0838 - val_loss: 1.6327 - val_categorical_accuracy: 0.1288 - val_auc: 0.6210 - val_recall: 0.0191\nEpoch 9/10\n9/9 [==============================] - 2s 191ms/step - loss: 1.2013 - categorical_accuracy: 0.4760 - auc: 0.7822 - recall: 0.1141 - val_loss: 1.3361 - val_categorical_accuracy: 0.4265 - val_auc: 0.7622 - val_recall: 0.0525\nEpoch 10/10\n9/9 [==============================] - 2s 187ms/step - loss: 1.0994 - categorical_accuracy: 0.5407 - auc: 0.8057 - recall: 0.1640 - val_loss: 1.4096 - val_categorical_accuracy: 0.4008 - val_auc: 0.7285 - val_recall: 0.0181\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[2 1 3 ... 3 1 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.937593984962406, 1: 1.9541044776119403, 2: 1.7934931506849314, 3: 0.32517851598882336, 4: 1.6625396825396825}\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet50 (Functional)        (None, 4, 4, 2048)        23587712  \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 32768)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               4194432   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 27,782,789\nTrainable params: 4,195,077\nNon-trainable params: 23,587,712\n_________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 5s 546ms/step - loss: 167.7130 - categorical_accuracy: 0.1237 - auc_1: 0.4440 - recall_1: 0.1213 - val_loss: 79.8907 - val_categorical_accuracy: 0.0630 - val_auc_1: 0.4144 - val_recall_1: 0.0630\nEpoch 2/10\n9/9 [==============================] - 3s 309ms/step - loss: 16.3510 - categorical_accuracy: 0.3111 - auc_1: 0.4817 - recall_1: 0.0408 - val_loss: 1.6044 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 3/10\n9/9 [==============================] - 3s 307ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6041 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 4/10\n9/9 [==============================] - 3s 300ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6041 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 5/10\n9/9 [==============================] - 3s 308ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6039 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 6/10\n9/9 [==============================] - 3s 306ms/step - loss: 1.5938 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6036 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 7/10\n9/9 [==============================] - 3s 304ms/step - loss: 1.5938 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6029 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 8/10\n9/9 [==============================] - 3s 310ms/step - loss: 1.5938 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6032 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 9/10\n9/9 [==============================] - 3s 303ms/step - loss: 1.5938 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6032 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 10/10\n9/9 [==============================] - 3s 308ms/step - loss: 1.5938 - categorical_accuracy: 0.6171 - auc_1: 0.7397 - recall_1: 0.0000e+00 - val_loss: 1.6032 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7015 - val_recall_1: 0.0000e+00\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[3 2 2 ... 4 1 2] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.9410852713178293, 1: 1.9553846153846153, 2: 1.7932980599647266, 3: 0.3252719129878439, 4: 1.6587275693311583}\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Functional)           (None, 4, 4, 512)         14714688  \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 128)               1048704   \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 15,764,037\nTrainable params: 1,049,349\nNon-trainable params: 14,714,688\n_________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 5s 520ms/step - loss: 6.3717 - categorical_accuracy: 0.2412 - auc_2: 0.5309 - recall_2: 0.2127 - val_loss: 2.0574 - val_categorical_accuracy: 0.1209 - val_auc_2: 0.5421 - val_recall_2: 0.1209\nEpoch 2/10\n9/9 [==============================] - 3s 360ms/step - loss: 2.3682 - categorical_accuracy: 0.2100 - auc_2: 0.5019 - recall_2: 0.1183 - val_loss: 1.9148 - val_categorical_accuracy: 0.1072 - val_auc_2: 0.3927 - val_recall_2: 0.0000e+00\nEpoch 3/10\n9/9 [==============================] - 3s 356ms/step - loss: 1.8351 - categorical_accuracy: 0.1879 - auc_2: 0.5067 - recall_2: 0.0300 - val_loss: 1.7514 - val_categorical_accuracy: 0.1209 - val_auc_2: 0.5329 - val_recall_2: 0.0000e+00\nEpoch 4/10\n9/9 [==============================] - 3s 347ms/step - loss: 1.6575 - categorical_accuracy: 0.2228 - auc_2: 0.5173 - recall_2: 0.0108 - val_loss: 1.5844 - val_categorical_accuracy: 0.1239 - val_auc_2: 0.5352 - val_recall_2: 0.0000e+00\nEpoch 5/10\n9/9 [==============================] - 3s 360ms/step - loss: 1.5891 - categorical_accuracy: 0.1586 - auc_2: 0.5041 - recall_2: 0.0000e+00 - val_loss: 1.5812 - val_categorical_accuracy: 0.1219 - val_auc_2: 0.5284 - val_recall_2: 0.0000e+00\nEpoch 6/10\n9/9 [==============================] - 3s 358ms/step - loss: 1.5890 - categorical_accuracy: 0.1156 - auc_2: 0.4806 - recall_2: 0.0000e+00 - val_loss: 1.6090 - val_categorical_accuracy: 0.1209 - val_auc_2: 0.4506 - val_recall_2: 0.0000e+00\nEpoch 7/10\n9/9 [==============================] - 3s 365ms/step - loss: 1.5864 - categorical_accuracy: 0.1205 - auc_2: 0.4502 - recall_2: 0.0000e+00 - val_loss: 1.6079 - val_categorical_accuracy: 0.1209 - val_auc_2: 0.5000 - val_recall_2: 0.0000e+00\nEpoch 8/10\n9/9 [==============================] - 3s 351ms/step - loss: 1.5864 - categorical_accuracy: 0.1205 - auc_2: 0.4861 - recall_2: 0.0000e+00 - val_loss: 1.6072 - val_categorical_accuracy: 0.1209 - val_auc_2: 0.4506 - val_recall_2: 0.0000e+00\nEpoch 9/10\n9/9 [==============================] - 3s 356ms/step - loss: 1.5864 - categorical_accuracy: 0.1901 - auc_2: 0.5234 - recall_2: 0.0000e+00 - val_loss: 1.6073 - val_categorical_accuracy: 0.1209 - val_auc_2: 0.4506 - val_recall_2: 0.0000e+00\nEpoch 10/10\n9/9 [==============================] - 3s 354ms/step - loss: 1.5864 - categorical_accuracy: 0.3042 - auc_2: 0.5466 - recall_2: 0.0000e+00 - val_loss: 1.6073 - val_categorical_accuracy: 0.1209 - val_auc_2: 0.4506 - val_recall_2: 0.0000e+00\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[2 3 3 ... 2 2 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.9410852713178293, 1: 1.9553846153846153, 2: 1.7932980599647266, 3: 0.32516789254876877, 4: 1.6614379084967321}\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninception_v3 (Functional)    (None, 2, 2, 2048)        21802784  \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 128)               1048704   \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 22,852,133\nTrainable params: 1,049,349\nNon-trainable params: 21,802,784\n_________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 4s 479ms/step - loss: 7.6196 - categorical_accuracy: 0.2702 - auc_3: 0.5652 - recall_3: 0.2370 - val_loss: 2.1095 - val_categorical_accuracy: 0.1386 - val_auc_3: 0.3313 - val_recall_3: 0.0000e+00\nEpoch 2/10\n9/9 [==============================] - 2s 243ms/step - loss: 2.1223 - categorical_accuracy: 0.2719 - auc_3: 0.5363 - recall_3: 0.1721 - val_loss: 1.5584 - val_categorical_accuracy: 0.3088 - val_auc_3: 0.6349 - val_recall_3: 0.0924\nEpoch 3/10\n9/9 [==============================] - 2s 224ms/step - loss: 1.8377 - categorical_accuracy: 0.2823 - auc_3: 0.5308 - recall_3: 0.1443 - val_loss: 1.6006 - val_categorical_accuracy: 0.1996 - val_auc_3: 0.5919 - val_recall_3: 0.0118\nEpoch 4/10\n9/9 [==============================] - 2s 230ms/step - loss: 1.5347 - categorical_accuracy: 0.3469 - auc_3: 0.6259 - recall_3: 0.0418 - val_loss: 2.2163 - val_categorical_accuracy: 0.1160 - val_auc_3: 0.3312 - val_recall_3: 0.0069\nEpoch 5/10\n9/9 [==============================] - 2s 229ms/step - loss: 1.6136 - categorical_accuracy: 0.1335 - auc_3: 0.3770 - recall_3: 0.0071 - val_loss: 1.7434 - val_categorical_accuracy: 0.1180 - val_auc_3: 0.3061 - val_recall_3: 0.0000e+00\nEpoch 6/10\n9/9 [==============================] - 2s 227ms/step - loss: 1.5676 - categorical_accuracy: 0.2415 - auc_3: 0.4758 - recall_3: 9.8353e-04 - val_loss: 1.5111 - val_categorical_accuracy: 0.4376 - val_auc_3: 0.6785 - val_recall_3: 9.8328e-04\nEpoch 7/10\n9/9 [==============================] - 2s 219ms/step - loss: 1.5611 - categorical_accuracy: 0.4160 - auc_3: 0.6298 - recall_3: 0.0258 - val_loss: 1.4765 - val_categorical_accuracy: 0.5084 - val_auc_3: 0.7296 - val_recall_3: 0.0000e+00\nEpoch 8/10\n9/9 [==============================] - 2s 218ms/step - loss: 1.5545 - categorical_accuracy: 0.4556 - auc_3: 0.6632 - recall_3: 0.0052 - val_loss: 1.7388 - val_categorical_accuracy: 0.2419 - val_auc_3: 0.4237 - val_recall_3: 0.0000e+00\nEpoch 9/10\n9/9 [==============================] - 2s 229ms/step - loss: 1.5362 - categorical_accuracy: 0.4593 - auc_3: 0.6512 - recall_3: 0.0093 - val_loss: 1.6849 - val_categorical_accuracy: 0.2930 - val_auc_3: 0.4907 - val_recall_3: 0.0000e+00\nEpoch 10/10\n9/9 [==============================] - 2s 226ms/step - loss: 1.5353 - categorical_accuracy: 0.4094 - auc_3: 0.6436 - recall_3: 0.0120 - val_loss: 1.6019 - val_categorical_accuracy: 0.3736 - val_auc_3: 0.5616 - val_recall_3: 0.0000e+00\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef load_test_data(new_w,new_h):\n    path = '../input/cassava-leaf-disease-classification/test_images'\n    path_loop = r'../input/cassava-leaf-disease-classification/test_images/*.*'\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data = []\n    name_files = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        x_train = np.array(frame_rgb) / 255\n        x_train = x_train.reshape(-1, new_h, new_w, channels)\n        data.append([x_train])\n        name_files.append(name_file)\n    return data, name_files","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the CSV\ndata, name_files = load_test_data(IMG_SIZE_X,IMG_SIZE_Y)\npredictions = vgg16_model.predict(data)\npred_df = pd.DataFrame(predictions)\npred_list = pred_df.idxmax(axis=\"columns\")\npred_df = pd.DataFrame(zip(name_files, pred_list), columns = [\"image_id\", \"label\"])\npred_df.head()\npd.DataFrame(pred_df, columns=['predictions']).to_csv('submission.csv')","execution_count":17,"outputs":[{"output_type":"stream","text":"100%|██████████| 1/1 [00:00<00:00, 62.62it/s]\n","name":"stderr"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}