{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pylab as plt\nimport glob\nimport os\nimport random\nimport keras\nfrom keras import backend as K\nfrom keras import layers\nfrom keras import metrics\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.models import Sequential, Model\nfrom keras.utils import plot_model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport cv2\nfrom tqdm import tqdm\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom collections import Counter\nfrom pandas import DataFrame\nimport gc\nimport tensorflow as tf\nfrom sklearn.utils import class_weight\nfrom sklearn.utils.class_weight import compute_class_weight","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data_local(new_w,new_h):\n    train_labels = pd.read_csv(r'C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train.csv',sep=',')\n    path_loop = r\"C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train_images\\*.*\"\n    path = r\"C:\\Users\\aleja\\OneDrive\\Desktop\\Kaggle\\Leave Issues Competition\\train_images\"\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        label = train_labels.loc[train_labels['image_id'] == name_file,'label'].values\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        data.append([frame_rgb,label])\n    return data","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(new_w,new_h):\n    train_labels = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv', sep=',')\n    path = '../input/cassava-leaf-disease-classification/train_images'\n    path_loop = r'../input/cassava-leaf-disease-classification/train_images/*.*'\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        label = train_labels.loc[train_labels['image_id'] == name_file,'label'].values\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        data.append([frame_rgb,label])\n    return data\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change before submitting to Kaggle from load_data_local to load_data\nIMG_SIZE_X = 150\nIMG_SIZE_Y = 150\ndata = load_data(IMG_SIZE_X, IMG_SIZE_Y)","execution_count":5,"outputs":[{"output_type":"stream","text":"100%|██████████| 21397/21397 [06:33<00:00, 54.42it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split_modified(data):\n    x_data = []\n    y_data = []\n    \n    for feature, label in data:\n        x_data.append(feature)\n        y_data.append(label)\n        \n    X_data, X_test, Y_data, Y_test = sklearn.model_selection.train_test_split(x_data, y_data, stratify = y_data, test_size=0.1, random_state=42)\n    X_model_1, X_aux, Y_model_1, Y_aux = sklearn.model_selection.train_test_split(X_data, Y_data, stratify = Y_data, test_size=0.66, random_state=42)\n    X_model_2, X_model_3, Y_model_2, Y_model_3 = sklearn.model_selection.train_test_split(X_aux, Y_aux, stratify = Y_aux, test_size=0.5, random_state=42)\n    return X_test, Y_test, X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3\n    \n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preparation(X_data, Y_data, new_h, new_w, channels):\n    x_train,x_val,y_train,y_val = train_test_split(X_data,Y_data,stratify=Y_data, train_size=0.8, random_state=42)\n    x_train = np.array(x_train) / 255\n    x_val = np.array(x_val) / 255\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    x_train = x_train.reshape(-1, new_h, new_w, channels)\n    x_val = x_val.reshape(-1, new_h, new_w, channels)\n    y_train = np.concatenate(y_train, axis=0)\n    y_val = np.concatenate(y_val, axis=0)\n    return x_train, x_val, y_train, y_val","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode target variables\ndef encode_target_variable(y_train,y_val):\n\n    class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n    d_class_weights = dict(enumerate(class_weights))\n    print(d_class_weights)\n    onehot_encoder = OneHotEncoder(sparse=False)\n    y_train = y_train.reshape(-1,1)\n    y_val = y_val.reshape(-1,1)\n    y_train_onehot = onehot_encoder.fit_transform(y_train)\n    y_val_onehot = onehot_encoder.fit_transform(y_val)\n    return y_train_onehot, y_val_onehot, d_class_weights","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_0(input_shape):\n    input_img = Input(shape=input_shape)  \n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(input_img)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same')(x)\n    x = Dropout(0.3)(x)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    y = MaxPooling2D((2, 2), padding='same')(y)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(y)\n    y = MaxPooling2D((2, 2), padding='same')(y)\n    y = Conv2D(16, (3, 3), activation='relu', padding='same')(y)\n    y = Dropout(0.4)(y)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    z = MaxPooling2D((2, 2), padding='same')(z)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(z)\n    z = MaxPooling2D((2, 2), padding='same')(z)\n    z = Conv2D(16, (3, 3), activation='relu', padding='same')(z)\n\n    out = layers.concatenate([x, y, z])\n    out = layers.Flatten()(out)\n    out = Dense(16, activation='selu')(out)\n    out = Dense(5, activation='softmax')(out)\n    \n    model_f = Model(inputs=[input_img], outputs=[out])\n    model_f.summary()\n    return model_f","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_1(x_train,x_val,input_shape):\n    weights_path = '../input/weightsclimbers/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    resnet50 = ResNet50(weights = None ,include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y,3))\n    resnet50.load_weights(weights_path, by_name=True)\n    resnet50.trainable = False # remove if you want to retrain resnet weights\n    # resnet50.summary()\n    transfer_model_1 = Sequential() \n    transfer_model_1.add(resnet50)\n    transfer_model_1.add(Flatten())\n    transfer_model_1.add(Dense(128, activation='relu'))\n    transfer_model_1.add(Dropout(0.2))\n    transfer_model_1.add(Dense(5, activation='softmax'))\n    transfer_model_1.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.resnet50.preprocess_input(x_train_new)\n    x_val_new = keras.applications.resnet50.preprocess_input(x_val_new)\n    return transfer_model_1, x_train_new, x_val_new","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_2(x_train,x_val):\n    weights_path = '../input/weightsclimbers/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    vgg16 = VGG16(weights=None, include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y, 3))\n    vgg16.load_weights(weights_path, by_name=True)\n    vgg16.trainable = False # remove if you want to retrain vgg16 weights\n    # vgg16.summary()\n    transfer_model_2 = Sequential()\n    transfer_model_2.add(vgg16)\n    transfer_model_2.add(Flatten())\n    transfer_model_2.add(Dense(128, activation='relu'))\n    transfer_model_2.add(Dropout(0.2))\n    transfer_model_2.add(Dense(5, activation='softmax'))\n    transfer_model_2.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.vgg16.preprocess_input(x_train_new)\n    x_val_new = keras.applications.vgg16.preprocess_input(x_val_new)\n    return transfer_model_2, x_train_new, x_val_new","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_3(x_train,x_val):\n    weights_path = '../input/weightsclimbers/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    inceptionV3 = InceptionV3(weights=None, include_top=False, input_shape=(IMG_SIZE_X, IMG_SIZE_Y, 3))\n    inceptionV3.load_weights(weights_path, by_name=False)\n    inceptionV3.trainable = False # remove if you want to retrain rinceptionV3 weights\n    # inceptionV3.summary()\n    transfer_model_3 = Sequential()\n    transfer_model_3.add(inceptionV3)\n    transfer_model_3.add(Flatten())\n    transfer_model_3.add(Dense(128, activation='relu'))\n    transfer_model_3.add(Dropout(0.2))\n    transfer_model_3.add(Dense(5, activation='softmax'))\n    transfer_model_3.summary()\n    x_train_new = x_train\n    x_val_new = x_val\n    x_train_new = keras.applications.inception_v3.preprocess_input(x_train_new)\n    x_val_new = keras.applications.inception_v3.preprocess_input(x_val_new)\n    return transfer_model_3, x_train_new, x_val_new","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot,class_weights):\n    \n    batch_size = batch_size\n    epochs = epochs\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n              metrics=['categorical_accuracy',keras.metrics.AUC(),keras.metrics.Recall()])\n    \n    \n    history = model.fit(x_train_new, y_train_onehot, \\\n                              batch_size=batch_size, epochs=epochs, \\\n                              validation_split=0.2, verbose=1, shuffle=True, validation_data=(x_val_new, y_val_onehot),\n                       class_weight = class_weights)\n    return model","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Main function ######################\n\n\nnew_h = IMG_SIZE_X\nnew_w = IMG_SIZE_Y\nbatch_size = 500\nepochs = 10\nchannels = 3\n#Divide data into test and training/validation for three different models\nX_test, Y_test, X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3 = train_test_split_modified(data)\ntraining_data = [X_model_1, Y_model_1, X_model_2, Y_model_2, X_model_3, Y_model_3]\n#Check that the training datasets are correctly stratified \nY_model_1_df = DataFrame(Y_model_1,columns=['labels'])\n#print(Y_model_1_df.labels.value_counts())\nY_model_2_df = DataFrame(Y_model_2,columns=['labels'])\n#print(Y_model_2_df.labels.value_counts())\nY_model_3_df = DataFrame(Y_model_3,columns=['labels'])\n#print(Y_model_3_df.labels.value_counts())\n#deaseases explanation: {\"0\": \"Cassava Bacterial Blight (CBB)\", \n#\"1\": \"Cassava Brown Streak Disease (CBSD)\", \"2\": \"Cassava Green Mottle (CGM)\", \n#\"3\": \"Cassava Mosaic Disease (CMD)\", \"4\": \"Healthy\"}\n\n# #visualize each of the classes\n# fig = plt.figure(figsize=(10, 6))\n\n# for i in range(8):\n#     img = X_model_1[i]\n#     fig.add_subplot(2, 4, i+1)\n#     plt.imshow(img)\n#     plt.title(Y_model_1[i])\n    \n#Baseline model\nx_train, x_val, y_train, y_val = data_preparation(training_data[0], training_data[1], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ninput_shape = [new_h,new_w,channels]\ndel y_train, y_val\nbaseline_model = create_model_0(input_shape)\ngc.collect()\nbaseline_model = train_model(baseline_model, batch_size, epochs, x_train, x_val, y_train_onehot, y_val_onehot, class_weights)\n#Resnet50\nx_train, x_val, y_train, y_val = data_preparation(training_data[0], training_data[1], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\nresnet50_model, x_train_new, x_val_new = create_model_1(x_train,x_val,input_shape)\ndel x_train,x_val\ngc.collect()\nresnet50_model = train_model(resnet50_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)\n#vgg16\nx_train, x_val, y_train, y_val = data_preparation(training_data[2], training_data[3], new_h, new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\nvgg16_model, x_train_new, x_val_new = create_model_2(x_train,x_val)\ndel x_train,x_val\ngc.collect()\nvgg16_model = train_model(vgg16_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)\n#InceptionV3\nx_train, x_val, y_train, y_val = data_preparation(training_data[4], training_data[5], new_h,new_w, channels)\ny_train_onehot, y_val_onehot, class_weights = encode_target_variable(y_train,y_val)\ndel y_train, y_val\ninception_model, x_train_new, x_val_new = create_model_3(x_train,x_val)\ndel x_train,x_val\ngc.collect()\ninception_model = train_model(inception_model, batch_size, epochs, x_train_new, x_val_new, y_train_onehot, y_val_onehot, class_weights)","execution_count":14,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[2 1 3 ... 3 1 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.937593984962406, 1: 1.9541044776119403, 2: 1.7934931506849314, 3: 0.32517851598882336, 4: 1.6625396825396825}\nModel: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 150, 150, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 150, 150, 16) 1216        input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 150, 150, 16) 448         input_1[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 75, 75, 16)   0           conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 75, 75, 16)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 150, 150, 16) 448         input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 75, 75, 16)   6416        max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 75, 75, 16)   2320        max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 75, 75, 16)   0           conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 38, 38, 16)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 38, 38, 16)   0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 75, 75, 16)   2320        max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 38, 38, 16)   6416        max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 38, 38, 16)   2320        max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 38, 38, 16)   0           conv2d_7[0][0]                   \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 38, 38, 16)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 38, 38, 16)   0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 38, 38, 16)   2320        max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 38, 38, 48)   0           dropout[0][0]                    \n                                                                 dropout_1[0][0]                  \n                                                                 conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 69312)        0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 16)           1109008     flatten[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 5)            85          dense[0][0]                      \n==================================================================================================\nTotal params: 1,133,317\nTrainable params: 1,133,317\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 4s 395ms/step - loss: 3.9950 - categorical_accuracy: 0.2242 - auc: 0.5376 - recall: 0.1086 - val_loss: 1.5769 - val_categorical_accuracy: 0.1431 - val_auc: 0.6259 - val_recall: 0.0000e+00\nEpoch 2/10\n9/9 [==============================] - 2s 264ms/step - loss: 1.5731 - categorical_accuracy: 0.1886 - auc: 0.5700 - recall: 0.0026 - val_loss: 1.5541 - val_categorical_accuracy: 0.1193 - val_auc: 0.6261 - val_recall: 0.0000e+00\nEpoch 3/10\n9/9 [==============================] - 2s 260ms/step - loss: 1.5174 - categorical_accuracy: 0.3005 - auc: 0.6388 - recall: 0.0026 - val_loss: 1.5601 - val_categorical_accuracy: 0.2548 - val_auc: 0.6328 - val_recall: 0.0029\nEpoch 4/10\n9/9 [==============================] - 2s 254ms/step - loss: 1.5282 - categorical_accuracy: 0.2986 - auc: 0.6097 - recall: 0.0389 - val_loss: 1.7909 - val_categorical_accuracy: 0.1269 - val_auc: 0.4818 - val_recall: 0.0286\nEpoch 5/10\n9/9 [==============================] - 2s 251ms/step - loss: 1.5188 - categorical_accuracy: 0.3698 - auc: 0.6619 - recall: 0.0878 - val_loss: 1.5299 - val_categorical_accuracy: 0.2395 - val_auc: 0.6432 - val_recall: 0.0000e+00\nEpoch 6/10\n9/9 [==============================] - 2s 266ms/step - loss: 1.3795 - categorical_accuracy: 0.4101 - auc: 0.7233 - recall: 0.0196 - val_loss: 1.5215 - val_categorical_accuracy: 0.2452 - val_auc: 0.6490 - val_recall: 0.0019\nEpoch 7/10\n9/9 [==============================] - 2s 267ms/step - loss: 1.2647 - categorical_accuracy: 0.5099 - auc: 0.7718 - recall: 0.0723 - val_loss: 2.8378 - val_categorical_accuracy: 0.0945 - val_auc: 0.4470 - val_recall: 0.0573\nEpoch 8/10\n9/9 [==============================] - 2s 261ms/step - loss: 1.3970 - categorical_accuracy: 0.4096 - auc: 0.6935 - recall: 0.0800 - val_loss: 1.8391 - val_categorical_accuracy: 0.1584 - val_auc: 0.4065 - val_recall: 0.0076\nEpoch 9/10\n9/9 [==============================] - 2s 265ms/step - loss: 1.1179 - categorical_accuracy: 0.5295 - auc: 0.7860 - recall: 0.1750 - val_loss: 1.2176 - val_categorical_accuracy: 0.6088 - val_auc: 0.8079 - val_recall: 0.3578\nEpoch 10/10\n9/9 [==============================] - 2s 252ms/step - loss: 1.1160 - categorical_accuracy: 0.5230 - auc: 0.7764 - recall: 0.2762 - val_loss: 1.7676 - val_categorical_accuracy: 0.1956 - val_auc: 0.5127 - val_recall: 0.0258\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[2 1 3 ... 3 1 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.937593984962406, 1: 1.9541044776119403, 2: 1.7934931506849314, 3: 0.32517851598882336, 4: 1.6625396825396825}\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet50 (Functional)        (None, 5, 5, 2048)        23587712  \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 51200)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               6553728   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 30,142,085\nTrainable params: 6,554,373\nNon-trainable params: 23,587,712\n_________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 6s 698ms/step - loss: 265.0516 - categorical_accuracy: 0.1442 - auc_1: 0.4594 - recall_1: 0.1442 - val_loss: 72.4126 - val_categorical_accuracy: 0.1155 - val_auc_1: 0.4472 - val_recall_1: 0.1155\nEpoch 2/10\n9/9 [==============================] - 4s 443ms/step - loss: 17.3739 - categorical_accuracy: 0.3321 - auc_1: 0.4958 - recall_1: 0.0625 - val_loss: 1.6064 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 3/10\n9/9 [==============================] - 4s 452ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6062 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 4/10\n9/9 [==============================] - 4s 471ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6060 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 5/10\n9/9 [==============================] - 4s 456ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6056 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 6/10\n9/9 [==============================] - 4s 452ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6060 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 7/10\n9/9 [==============================] - 4s 451ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7607 - recall_1: 0.0000e+00 - val_loss: 1.6053 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 8/10\n9/9 [==============================] - 4s 453ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7579 - recall_1: 0.0000e+00 - val_loss: 1.6052 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7543 - val_recall_1: 0.0000e+00\nEpoch 9/10\n9/9 [==============================] - 4s 450ms/step - loss: 1.5939 - categorical_accuracy: 0.6171 - auc_1: 0.7465 - recall_1: 0.0000e+00 - val_loss: 1.6055 - val_categorical_accuracy: 0.6069 - val_auc_1: 0.7015 - val_recall_1: 0.0000e+00\nEpoch 10/10\n9/9 [==============================] - 4s 447ms/step - loss: 1.5938 - categorical_accuracy: 0.6171 - auc_1: 0.7116 - recall_1: 0.0000e+00 - val_loss: 1.6057 - val_categorical_accuracy: 0.1155 - val_auc_1: 0.7015 - val_recall_1: 0.0000e+00\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[3 2 2 ... 4 1 2] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.9410852713178293, 1: 1.9553846153846153, 2: 1.7932980599647266, 3: 0.3252719129878439, 4: 1.6587275693311583}\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Functional)           (None, 4, 4, 512)         14714688  \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 128)               1048704   \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 15,764,037\nTrainable params: 1,049,349\nNon-trainable params: 14,714,688\n_________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 7s 747ms/step - loss: 3.3390 - categorical_accuracy: 0.2149 - auc_2: 0.5009 - recall_2: 0.1500 - val_loss: 3.2211 - val_categorical_accuracy: 0.1209 - val_auc_2: 0.4941 - val_recall_2: 0.1209\nEpoch 2/10\n9/9 [==============================] - 5s 525ms/step - loss: 1.9827 - categorical_accuracy: 0.2053 - auc_2: 0.5199 - recall_2: 0.0575 - val_loss: 2.3019 - val_categorical_accuracy: 0.0600 - val_auc_2: 0.3566 - val_recall_2: 0.0600\nEpoch 3/10\n9/9 [==============================] - 5s 536ms/step - loss: 1.7212 - categorical_accuracy: 0.1765 - auc_2: 0.5149 - recall_2: 0.0103 - val_loss: 2.0649 - val_categorical_accuracy: 0.0600 - val_auc_2: 0.3587 - val_recall_2: 0.0000e+00\nEpoch 4/10\n9/9 [==============================] - 5s 542ms/step - loss: 1.6681 - categorical_accuracy: 0.2127 - auc_2: 0.5151 - recall_2: 2.4588e-04 - val_loss: 1.5162 - val_categorical_accuracy: 0.5900 - val_auc_2: 0.7792 - val_recall_2: 0.0000e+00\nEpoch 5/10\n9/9 [==============================] - 5s 533ms/step - loss: 1.5875 - categorical_accuracy: 0.3339 - auc_2: 0.5884 - recall_2: 0.0000e+00 - val_loss: 1.5375 - val_categorical_accuracy: 0.3235 - val_auc_2: 0.7200 - val_recall_2: 0.0000e+00\nEpoch 6/10\n9/9 [==============================] - 5s 524ms/step - loss: 1.5860 - categorical_accuracy: 0.2048 - auc_2: 0.5541 - recall_2: 0.0000e+00 - val_loss: 1.9466 - val_categorical_accuracy: 0.0600 - val_auc_2: 0.2497 - val_recall_2: 0.0000e+00\nEpoch 7/10\n9/9 [==============================] - 5s 537ms/step - loss: 1.5985 - categorical_accuracy: 0.3182 - auc_2: 0.4925 - recall_2: 0.0000e+00 - val_loss: 1.5054 - val_categorical_accuracy: 0.5900 - val_auc_2: 0.7684 - val_recall_2: 0.0000e+00\nEpoch 8/10\n9/9 [==============================] - 5s 532ms/step - loss: 1.5939 - categorical_accuracy: 0.5004 - auc_2: 0.6331 - recall_2: 0.0000e+00 - val_loss: 1.5909 - val_categorical_accuracy: 0.5900 - val_auc_2: 0.7377 - val_recall_2: 0.0000e+00\nEpoch 9/10\n9/9 [==============================] - 5s 537ms/step - loss: 1.5861 - categorical_accuracy: 0.6206 - auc_2: 0.6521 - recall_2: 0.0000e+00 - val_loss: 1.6077 - val_categorical_accuracy: 0.5900 - val_auc_2: 0.5174 - val_recall_2: 0.0000e+00\nEpoch 10/10\n9/9 [==============================] - 5s 527ms/step - loss: 1.5859 - categorical_accuracy: 0.6093 - auc_2: 0.6430 - recall_2: 0.0000e+00 - val_loss: 1.5681 - val_categorical_accuracy: 0.5900 - val_auc_2: 0.7604 - val_recall_2: 0.0000e+00\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=[2 3 3 ... 2 2 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"{0: 3.9410852713178293, 1: 1.9553846153846153, 2: 1.7932980599647266, 3: 0.32516789254876877, 4: 1.6614379084967321}\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninception_v3 (Functional)    (None, 3, 3, 2048)        21802784  \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 18432)             0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 128)               2359424   \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 24,162,853\nTrainable params: 2,360,069\nNon-trainable params: 21,802,784\n_________________________________________________________________\nEpoch 1/10\n9/9 [==============================] - 5s 536ms/step - loss: 2.0287 - categorical_accuracy: 0.2356 - auc_3: 0.6153 - recall_3: 0.0551 - val_loss: 1.6695 - val_categorical_accuracy: 0.1672 - val_auc_3: 0.5486 - val_recall_3: 0.0029\nEpoch 2/10\n9/9 [==============================] - 3s 291ms/step - loss: 1.5256 - categorical_accuracy: 0.3715 - auc_3: 0.6687 - recall_3: 0.0433 - val_loss: 1.4506 - val_categorical_accuracy: 0.3717 - val_auc_3: 0.6848 - val_recall_3: 0.0403\nEpoch 3/10\n9/9 [==============================] - 3s 282ms/step - loss: 1.4719 - categorical_accuracy: 0.4047 - auc_3: 0.6986 - recall_3: 0.0642 - val_loss: 1.3474 - val_categorical_accuracy: 0.5526 - val_auc_3: 0.7766 - val_recall_3: 0.0747\nEpoch 4/10\n9/9 [==============================] - 3s 294ms/step - loss: 1.4605 - categorical_accuracy: 0.4337 - auc_3: 0.7194 - recall_3: 0.0794 - val_loss: 1.4632 - val_categorical_accuracy: 0.3392 - val_auc_3: 0.6886 - val_recall_3: 0.0521\nEpoch 5/10\n9/9 [==============================] - 3s 291ms/step - loss: 1.4485 - categorical_accuracy: 0.4281 - auc_3: 0.7293 - recall_3: 0.0969 - val_loss: 1.3009 - val_categorical_accuracy: 0.5506 - val_auc_3: 0.7804 - val_recall_3: 0.1485\nEpoch 6/10\n9/9 [==============================] - 3s 283ms/step - loss: 1.4157 - categorical_accuracy: 0.4468 - auc_3: 0.7338 - recall_3: 0.1050 - val_loss: 1.3100 - val_categorical_accuracy: 0.5034 - val_auc_3: 0.7700 - val_recall_3: 0.1750\nEpoch 7/10\n9/9 [==============================] - 3s 286ms/step - loss: 1.4069 - categorical_accuracy: 0.4433 - auc_3: 0.7433 - recall_3: 0.1288 - val_loss: 1.3782 - val_categorical_accuracy: 0.4287 - val_auc_3: 0.7382 - val_recall_3: 0.0806\nEpoch 8/10\n9/9 [==============================] - 3s 287ms/step - loss: 1.3318 - categorical_accuracy: 0.4812 - auc_3: 0.7757 - recall_3: 0.1222 - val_loss: 1.5510 - val_categorical_accuracy: 0.2852 - val_auc_3: 0.6300 - val_recall_3: 0.0147\nEpoch 9/10\n9/9 [==============================] - 3s 285ms/step - loss: 1.3482 - categorical_accuracy: 0.4411 - auc_3: 0.7586 - recall_3: 0.1215 - val_loss: 1.2858 - val_categorical_accuracy: 0.4828 - val_auc_3: 0.7842 - val_recall_3: 0.1131\nEpoch 10/10\n9/9 [==============================] - 2s 277ms/step - loss: 1.3270 - categorical_accuracy: 0.4593 - auc_3: 0.7598 - recall_3: 0.1379 - val_loss: 1.3583 - val_categorical_accuracy: 0.4454 - val_auc_3: 0.7485 - val_recall_3: 0.1278\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef load_test_data(new_w,new_h):\n    path = '../input/cassava-leaf-disease-classification/test_images'\n    path_loop = r'../input/cassava-leaf-disease-classification/test_images/*.*'\n    onlyfiles = next(os.walk(path))[2] #dir is your directory path as string\n    numOfFiles = len(onlyfiles)\n    data_test = []\n    name_files = []\n    for file in tqdm(glob.glob(path_loop)):\n        a=cv2.imread(file)\n        name_file = os.path.basename(file)\n        #conversion numpy array into rgb image to show\n        c = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n        h, w, channels = c.shape\n        #input size of Resnet architecture\n        frame_rgb = cv2.resize(c,(new_w,new_h),interpolation=cv2.INTER_CUBIC)\n        x_train = np.array(frame_rgb) / 255\n        x_train = x_train.reshape(-1, new_h, new_w, channels)\n        data_test.append([x_train])\n        name_files.append(name_file)\n    return data_test, name_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#keras.models.save_model(vgg16_model,'../output/kaggle/working/')","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Create the CSV\n# data_test, name_files = load_test_data(IMG_SIZE_X,IMG_SIZE_Y)\n# predictions = vgg16_model.predict(data_test)\n# pred_df = pd.DataFrame(predictions)\n# pred_list = pred_df.idxmax(axis=\"columns\")\n# pred_df = pd.DataFrame(zip(name_files, pred_list), columns = [\"image_id\", \"label\"])\n# print(pred_df)\n# pred_df.to_csv('submission.csv',index=False)\n# !head submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\ndirec = \"../input/cassava-leaf-disease-classification/\"\ntest_direc = direc + \"test_images/\"\nsample_sub_csv = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n\nfor image in sample_sub_csv.image_id:\n    img = keras.preprocessing.image.load_img('../input/cassava-leaf-disease-classification/test_images/' + image)\n    img = keras.preprocessing.image.img_to_array(img)\n    img = keras.preprocessing.image.smart_resize(img, (IMG_SIZE_X, IMG_SIZE_Y))\n    img = tf.reshape(img, (-1, IMG_SIZE_X, IMG_SIZE_Y, 3))\n    prediction = vgg16_model.predict(img/255)\n    preds.append(np.argmax(prediction))\n\nfinal_submission = pd.DataFrame({'image_id': sample_sub_csv.image_id, 'label': preds})\nfinal_submission.to_csv('submission.csv', index=False) ","execution_count":31,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}